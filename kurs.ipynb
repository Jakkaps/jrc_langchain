{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JrC Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.cache import InMemoryCache\n",
    "langchain.llm_cache = InMemoryCache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Used to standardize the interface to functionality\n",
    "- Thus, we can compose different chains together to achieve new functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from langchain.schema.language_model import BaseLanguageModel\n",
    "from langchain.callbacks.manager import CallbackManagerForChainRun\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "class LinkedInChain(Chain):\n",
    "    prompt_template = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        Generate a LinkedIn post that starts with \"I'm proud and humble to...\" and follows the typical format and tone of clichéd corporate announcements. Make the post about {activity} for a company named Junior Consulting.\n",
    "        \"\"\"\n",
    "    )\n",
    "    llm: BaseLanguageModel\n",
    "    output_key: str = \"post\"\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        return self.prompt_template.input_variables\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return [self.output_key]\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def _chain_type(self) -> str:\n",
    "        return \"linkedin_chain\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        prompt_value = self.prompt_template.format_prompt(**inputs)\n",
    "        response = self.llm.generate_prompt(\n",
    "            [prompt_value], callbacks=run_manager.get_child() if run_manager else None\n",
    "        )\n",
    "\n",
    "        return {self.output_key: response.generations[0][0].text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LinkedInChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      " \"I'm proud and humbled to announce that this morning, I had the absolute privilege of cooking breakfast for the brilliant minds at Junior Consulting. \n",
      "In the early hours, as the sun was rising, I found myself flipping pancakes, scrambling eggs, and brewing fresh coffee, all in the name of fostering team spirit and fueling productivity. It was an exercise in teamwork, communication, and the art of multitasking - skills that I apply daily in my professional life. \n",
      "I was reminded that it’s not just about the end product, but the journey you take to get there. The laughter, the camaraderie, the shared anticipation of a well-cooked meal – these are the moments that truly build a team. \n",
      "Thank you, Junior Consulting, for allowing me to serve you in a different capacity today. I am constantly inspired by your dedication, your passion, and your hunger for success (and pancakes).\n",
      "Here's to more mornings filled with laughter, team spirit, and of course, delicious breakfast! #teamwork #leadership #corporateculture #breakfastclub\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.stdout import StdOutCallbackHandler\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "\n",
    "linkedin_chain = LinkedInChain(\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\"),\n",
    ")\n",
    "\n",
    "post = linkedin_chain.run({\"activity\": \"successfully cooking breakfast\"}, callbacks=[StdOutCallbackHandler()])\n",
    "print(\"\\n\", post.replace(\"\\n\\n\", \"\\n\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To allow LLMs to to take actions\n",
    "- Actions can be arbitrary, like calling APIs, reading files, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "class LinkedInChainInput(BaseModel):\n",
    "    activity: str = Field()\n",
    "\n",
    "tools = []\n",
    "\n",
    "class LinkedInChainInput(BaseModel):\n",
    "    activity: str = Field()\n",
    "\n",
    "tools.append(\n",
    "    Tool.from_function(\n",
    "        func=linkedin_chain.run,\n",
    "        name=\"linkedin_post_generator\",\n",
    "        description=\"To generate a LinkedIn post about a specified activity\",\n",
    "        args_schema=LinkedInChainInput\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from Levenshtein import distance\n",
    "\n",
    "class ConsultantInfoInput(BaseModel):\n",
    "    name: str = Field(description=\"The name of the consultant you want information about\")\n",
    "\n",
    "def consultant_info(name: str) -> str:\n",
    "    consultants = json.load(open(\"ansatte.json\"))\n",
    "    \n",
    "    best_match_ld = float(\"inf\")  \n",
    "    best_match = None\n",
    "\n",
    "    for consultant in consultants:\n",
    "        consultant_name = consultant[\"first_name\"]\n",
    "        if name.count(\" \") > 1:\n",
    "            consultant_name += consultant[\"last_name\"]\n",
    "\n",
    "        ld = distance(name.lower(), consultant_name.lower())\n",
    "        if ld < best_match_ld:\n",
    "            best_match_ld = ld\n",
    "            best_match = consultant\n",
    "\n",
    "    return best_match\n",
    "\n",
    "tools.append(\n",
    "    Tool.from_function(\n",
    "        func=consultant_info,\n",
    "        name=\"consultant_info\",\n",
    "        description=\"To get info about a consultant\",\n",
    "        args_schema=ConsultantInfoInput\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Junior Consulting Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, OpenAIFunctionsAgent\n",
    "from langchain.schema import SystemMessage\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "prompt = OpenAIFunctionsAgent.create_prompt(\n",
    "    system_message=SystemMessage(content=\"You are a chatbot that helps consultants in Junior Consulting do various tasks.\"),\n",
    ")\n",
    "\n",
    "agent = OpenAIFunctionsAgent(\n",
    "    llm=llm, tools=tools, verbose=True, prompt=prompt\n",
    ")\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe answer is 4.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The answer is 4.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is 2+2?\"\n",
    "answer = agent_executor.run(question)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
